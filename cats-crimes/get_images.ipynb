{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify images of cats and elephants. Using webscraping! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get images of cat.\n",
    "\n",
    "It would be easy to do this using an [API](https://thecatapi.com/) or just download these images via a [Kaggle dataset](https://www.kaggle.com/competitions/dogs-vs-cats/data), but I decided to do webscraping as a good tutorial on how to do it. \n",
    "\n",
    "Simply, what we need to do is a few things. \n",
    "\n",
    "We need to \n",
    "1. Get the url, and extract the HTML from it. \n",
    "2. Parse it using beautiful soup to find the images\n",
    "3. Pre-process and or standardize the data\n",
    "4. Save the images ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_images = []\n",
    "\n",
    "\n",
    "for i in range(1, 30):\n",
    "    baseurl = \"https://pixabay.com/photos/search/cat/\" + \"?pagi=\" + str(i)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(baseurl)\n",
    "    time.sleep(.5)\n",
    "    html = driver.page_source\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "\n",
    "    for item in soup.find_all('img'):\n",
    "        cat_images.append(item['src'])\n",
    "\n",
    "\n",
    "#Remove all gifs, they are not images. Include jpg and png\n",
    "cat_images = [x for x in cat_images if x.endswith('.jpg') or x.endswith('.png')]\n",
    "\n",
    "# Print the list of cat images\n",
    "#print(cat_images)\n",
    "\n",
    "# Save the cat images to a folder\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "for i, url in enumerate(cat_images):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # be sure that it ends with .jpg or .png\n",
    "        with open('cat_images/cat_image_' + str(i) + '.jpg', 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to go get the elephant images! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephant_images = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    baseurl = \"https://pixabay.com/photos/search/elephant/\" + \"?pagi=\" + str(i)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(baseurl)\n",
    "    time.sleep(.5)\n",
    "    html = driver.page_source\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "\n",
    "    for item in soup.find_all('img'):\n",
    "        elephant_images.append(item['src'])\n",
    "\n",
    "\n",
    "#Remove all gifs, they are not images. Include jpg and png\n",
    "elephant_images = [x for x in elephant_images if x.endswith('.jpg') or x.endswith('.png')]\n",
    "\n",
    "# Print the list of cat images\n",
    "#print(cat_images)\n",
    "\n",
    "# Save the cat images to a folder\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "for i, url in enumerate(elephant_images):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # be sure that it ends with .jpg or .png\n",
    "        with open('elephant_images/elephant_image_' + str(i) + '.jpg', 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "167171df70846b30a8d0159733df1712935e672ad1299b7ff9f875d0ba54fe24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
